{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7311cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the two input images\n",
    "image_bad = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\")\n",
    "image_good = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2478.jpg\")\n",
    "\n",
    "# Convert the images to the Lab color space\n",
    "image_bad_lab = cv2.cvtColor(image_bad, cv2.COLOR_BGR2LAB)\n",
    "image_good_lab = cv2.cvtColor(image_good, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split the Lab images into separate channels\n",
    "l_channel_bad, a_channel_bad, b_channel_bad = cv2.split(image_bad_lab)\n",
    "l_channel_good, a_channel_good, b_channel_good = cv2.split(image_good_lab)\n",
    "\n",
    "# Perform histogram matching on the L channel\n",
    "l_channel_matched = cv2.createCLAHE().apply(l_channel_bad)\n",
    "l_channel_matched = cv2.equalizeHist(l_channel_matched)\n",
    "\n",
    "# Transfer the color and contrast information from the good image to the matched L channel\n",
    "a_channel_matched = a_channel_good\n",
    "b_channel_matched = b_channel_good\n",
    "\n",
    "# Merge the matched channels back into the Lab image\n",
    "image_matched_lab = cv2.merge([l_channel_matched, a_channel_matched, b_channel_matched])\n",
    "\n",
    "# Convert the matched Lab image back to BGR color space\n",
    "image_matched = cv2.cvtColor(image_matched_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the matched image\n",
    "cv2.imshow('Matched Image', image_matched)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99060b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_loss() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m style_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSC_2478.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Run the style transfer algorithm\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mrun_style_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Save and display the result image\u001b[39;00m\n\u001b[0;32m    107\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(result_image)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[1;34m(content_path, style_path, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[0;32m     73\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_features\u001b[39m\u001b[38;5;124m'\u001b[39m: content_features,\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_features\u001b[39m\u001b[38;5;124m'\u001b[39m: style_features,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m: generated_image,\n\u001b[0;32m     78\u001b[0m }\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Compute gradients and loss\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     grads, all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m content_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m style_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Update the generated image using the optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m, in \u001b[0;36mcompute_grads\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_grads\u001b[39m(cfg):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 54\u001b[0m         all_loss \u001b[38;5;241m=\u001b[39m compute_loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[0;32m     55\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m], cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads, all_loss\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_loss() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the VGG19 model pre-trained on ImageNet\n",
    "model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# Define the content and style layers for style transfer\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load and preprocess the input image\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # Resize the image to VGG19 input size\n",
    "    img = np.array(img)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Deprocess the image to convert it back to a valid image\n",
    "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
    "    img += [103.939, 116.779, 123.68]  # Undo VGG19 preprocessing\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "def get_feature_representations(model, content_path, style_path):\n",
    "    # Preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Extract the feature representations from the model\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs = model(style_image)\n",
    "\n",
    "    # Get the feature representations from the desired layers\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[len(content_layers):]]\n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[:len(style_layers)]]\n",
    "\n",
    "    return content_features, style_features\n",
    "\n",
    "def compute_loss(content_features, style_features, generated_features):\n",
    "    # Compute the content loss\n",
    "    content_loss = tf.add_n([tf.reduce_mean(tf.square(c - g)) for c, g in zip(content_features, generated_features)])\n",
    "\n",
    "    # Compute the style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean(tf.square(s - g)) for s, g in zip(style_features, generated_features)])\n",
    "\n",
    "    return content_loss, style_loss\n",
    "\n",
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(**cfg)\n",
    "    grads = tape.gradient(all_loss[0] + all_loss[1], cfg['generated_image'])\n",
    "    return grads, all_loss\n",
    "\n",
    "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
    "    # Load and preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Initialize the generated image as a copy of the content image\n",
    "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "    # Get the feature representations of the content and style images\n",
    "    content_features, style_features = get_feature_representations(model, content_path, style_path)\n",
    "\n",
    "    # Set the optimizer and learning rate\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    # Create a configuration dictionary for computing gradients and loss\n",
    "    cfg = {\n",
    "        'model': model,\n",
    "        'content_features': content_features,\n",
    "        'style_features': style_features,\n",
    "        'generated_image': generated_image,\n",
    "    }\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradients and loss\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss = content_weight * all_loss[0] + style_weight * all_loss[1]\n",
    "\n",
    "        # Update the generated image using the optimizer\n",
    "        optimizer.apply_gradients([(grads, generated_image)])\n",
    "\n",
    "        # Clip the generated image to valid range\n",
    "        generated_image.assign(tf.clip_by_value(generated_image, -1, 1))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration: {i}, Total loss: {loss:.2f}, Content loss: {all_loss[0]:.2f}, Style loss: {all_loss[1]:.2f}\")\n",
    "\n",
    "    # Deprocess the final generated image\n",
    "    final_image = deprocess_image(generated_image.numpy())\n",
    "\n",
    "    return final_image\n",
    "\n",
    "# Specify the paths to the content and style images\n",
    "content_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\"\n",
    "style_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2478.jpg\"\n",
    "\n",
    "# Run the style transfer algorithm\n",
    "result_image = run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2)\n",
    "\n",
    "# Save and display the result image\n",
    "Image.fromarray(result_image).save('output_image.jpg')\n",
    "Image.fromarray(result_image).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8803bb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 2 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m style_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSC_2478.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Run the style transfer algorithm\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mrun_style_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Save and display the result image\u001b[39;00m\n\u001b[0;32m    114\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(result_image)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 89\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[1;34m(content_path, style_path, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[0;32m     81\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_features\u001b[39m\u001b[38;5;124m'\u001b[39m: content_features,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_features\u001b[39m\u001b[38;5;124m'\u001b[39m: style_features,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m: generated_image,\n\u001b[0;32m     85\u001b[0m }\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# Compute gradients and loss\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     grads, all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m content_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m style_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# Update the generated image using the optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mcompute_grads\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_grads\u001b[39m(cfg):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 62\u001b[0m         all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstyle_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m], cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads, all_loss\n",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(content_features, style_features, generated_features)\u001b[0m\n\u001b[0;32m     45\u001b[0m style_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(style_features[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     46\u001b[0m generated_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(generated_features[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 47\u001b[0m style_features \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mreshape(style, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, style_shape[\u001b[38;5;241m1\u001b[39m], style_shape[\u001b[38;5;241m2\u001b[39m], style_shape[\u001b[38;5;241m3\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m style \u001b[38;5;129;01min\u001b[39;00m style_features]\n\u001b[0;32m     48\u001b[0m generated_features \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mreshape(generated, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, generated_shape[\u001b[38;5;241m1\u001b[39m], generated_shape[\u001b[38;5;241m2\u001b[39m], generated_shape[\u001b[38;5;241m3\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m generated \u001b[38;5;129;01min\u001b[39;00m generated_features]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compute the content loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m style_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(style_features[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     46\u001b[0m generated_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(generated_features[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 47\u001b[0m style_features \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mreshape(style, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, style_shape[\u001b[38;5;241m1\u001b[39m], \u001b[43mstyle_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, style_shape[\u001b[38;5;241m3\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m style \u001b[38;5;129;01min\u001b[39;00m style_features]\n\u001b[0;32m     48\u001b[0m generated_features \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mreshape(generated, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, generated_shape[\u001b[38;5;241m1\u001b[39m], generated_shape[\u001b[38;5;241m2\u001b[39m], generated_shape[\u001b[38;5;241m3\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m generated \u001b[38;5;129;01min\u001b[39;00m generated_features]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compute the content loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 2 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the VGG19 model pre-trained on ImageNet\n",
    "model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# Define the content and style layers for style transfer\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load and preprocess the input image\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # Resize the image to VGG19 input size\n",
    "    img = np.array(img)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Deprocess the image to convert it back to a valid image\n",
    "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
    "    img += [103.939, 116.779, 123.68]  # Undo VGG19 preprocessing\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "def get_feature_representations(model, content_path, style_path):\n",
    "    # Preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Extract the feature representations from the model\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs = model(style_image)\n",
    "\n",
    "    # Get the feature representations from the desired layers\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[len(content_layers):]]\n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[:len(style_layers)]]\n",
    "\n",
    "    return content_features, style_features\n",
    "\n",
    "def compute_loss(content_features, style_features, generated_features):\n",
    "    # Flatten the style and generated features\n",
    "    style_shape = tf.shape(style_features[0])\n",
    "    generated_shape = tf.shape(generated_features[0])\n",
    "    style_features = [tf.reshape(style, (-1, style_shape[1], style_shape[2], style_shape[3])) for style in style_features]\n",
    "    generated_features = [tf.reshape(generated, (-1, generated_shape[1], generated_shape[2], generated_shape[3])) for generated in generated_features]\n",
    "\n",
    "    # Compute the content loss\n",
    "    content_loss = tf.reduce_mean(tf.square(content_features - generated_features[-1]))\n",
    "\n",
    "    # Compute the style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean(tf.square(s - g)) for s, g in zip(style_features, generated_features[:-1])])\n",
    "\n",
    "    return content_loss, style_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(cfg['content_features'], cfg['style_features'], cfg['generated_image'])\n",
    "    grads = tape.gradient(all_loss[0] + all_loss[1], cfg['generated_image'])\n",
    "    return grads, all_loss\n",
    "\n",
    "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
    "    # Load and preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Initialize the generated image as a copy of the content image\n",
    "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "    # Get the feature representations of the content and style images\n",
    "    content_features, style_features = get_feature_representations(model, content_path, style_path)\n",
    "\n",
    "    # Set the optimizer and learning rate\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    # Create a configuration dictionary for computing gradients and loss\n",
    "    cfg = {\n",
    "        'content_features': content_features,\n",
    "        'style_features': style_features,\n",
    "        'generated_image': generated_image,\n",
    "    }\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradients and loss\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss = content_weight * all_loss[0] + style_weight * all_loss[1]\n",
    "\n",
    "        # Update the generated image using the optimizer\n",
    "        optimizer.apply_gradients([(grads, generated_image)])\n",
    "\n",
    "        # Clip the generated image to valid range\n",
    "        generated_image.assign(tf.clip_by_value(generated_image, -1, 1))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration: {i}, Total loss: {loss:.2f}, Content loss: {all_loss[0]:.2f}, Style loss: {all_loss[1]:.2f}\")\n",
    "\n",
    "    # Deprocess the final generated image\n",
    "    final_image = deprocess_image(generated_image.numpy())\n",
    "\n",
    "    return final_image\n",
    "\n",
    "# Specify the paths to the content and style images\n",
    "content_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\"\n",
    "style_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2478.jpg\"\n",
    "\n",
    "# Run the style transfer algorithm\n",
    "result_image = run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2)\n",
    "\n",
    "# Save and display the result image\n",
    "Image.fromarray(result_image).save('output_image.jpg')\n",
    "Image.fromarray(result_image).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab0b7c9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m style_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSC_2478.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Run the style transfer algorithm\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mrun_style_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Save and display the result image\u001b[39;00m\n\u001b[0;32m    112\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(result_image)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 87\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[1;34m(content_path, style_path, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[0;32m     79\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_features\u001b[39m\u001b[38;5;124m'\u001b[39m: content_features,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_features\u001b[39m\u001b[38;5;124m'\u001b[39m: style_features,\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m: generated_image,\n\u001b[0;32m     83\u001b[0m }\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Compute gradients and loss\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     grads, all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m content_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m style_weight \u001b[38;5;241m*\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# Update the generated image using the optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 60\u001b[0m, in \u001b[0;36mcompute_grads\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_grads\u001b[39m(cfg):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 60\u001b[0m         all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstyle_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(all_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m all_loss[\u001b[38;5;241m1\u001b[39m], cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads, all_loss\n",
      "Cell \u001b[1;32mIn[14], line 45\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(content_features, style_features, generated_features)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(content_features, style_features, generated_features):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Ensure content_features and generated_features have the same shape\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     target_shape \u001b[38;5;241m=\u001b[39m (tf\u001b[38;5;241m.\u001b[39mshape(content_features)[\u001b[38;5;241m0\u001b[39m], \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, tf\u001b[38;5;241m.\u001b[39mshape(content_features)[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     46\u001b[0m     target_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(content_features)\n\u001b[0;32m     47\u001b[0m     generated_features \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(generated_features, target_shape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the VGG19 model pre-trained on ImageNet\n",
    "model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# Define the content and style layers for style transfer\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # Load and preprocess the input image\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(target_size)  # Resize the image to the target size\n",
    "    img = np.array(img)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Deprocess the image to convert it back to a valid image\n",
    "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
    "    img += [103.939, 116.779, 123.68]  # Undo VGG19 preprocessing\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "def get_feature_representations(model, content_path, style_path):\n",
    "    # Preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Extract the feature representations from the model\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs = model(style_image)\n",
    "\n",
    "    # Get the feature representations from the desired layers\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[len(content_layers):]]\n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[:len(style_layers)]]\n",
    "\n",
    "    return content_features, style_features\n",
    "\n",
    "def compute_loss(content_features, style_features, generated_features):\n",
    "    # Ensure content_features and generated_features have the same shape\n",
    "    target_shape = (tf.shape(content_features)[0], tf.shape(content_features)[1], tf.shape(content_features)[2])\n",
    "    target_shape = tf.shape(content_features)\n",
    "    generated_features = tf.image.resize(generated_features, target_shape[1:3])\n",
    "\n",
    "    # Compute the content loss\n",
    "    content_loss = tf.reduce_mean(tf.square(content_features - generated_features))\n",
    "\n",
    "    # Compute the style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean(tf.square(s - g)) for s, g in zip(style_features, generated_features)])\n",
    "\n",
    "    return content_loss, style_loss\n",
    "\n",
    "\n",
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(cfg['content_features'], cfg['style_features'], cfg['generated_image'])\n",
    "    grads = tape.gradient(all_loss[0] + all_loss[1], cfg['generated_image'])\n",
    "    return grads, all_loss\n",
    "\n",
    "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
    "    # Load and preprocess the content and style images\n",
    "    content_image = preprocess_image(content_path)\n",
    "    style_image = preprocess_image(style_path)\n",
    "\n",
    "    # Initialize the generated image as a copy of the content image\n",
    "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "    # Get the feature representations of the content and style images\n",
    "    content_features, style_features = get_feature_representations(model, content_path, style_path)\n",
    "\n",
    "    # Set the optimizer and learning rate\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    # Create a configuration dictionary for computing gradients and loss\n",
    "    cfg = {\n",
    "        'content_features': content_features,\n",
    "        'style_features': style_features,\n",
    "        'generated_image': generated_image,\n",
    "    }\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradients and loss\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss = content_weight * all_loss[0] + style_weight * all_loss[1]\n",
    "\n",
    "        # Update the generated image using the optimizer\n",
    "        optimizer.apply_gradients([(grads, generated_image)])\n",
    "\n",
    "        # Clip the generated image to valid range\n",
    "        generated_image.assign(tf.clip_by_value(generated_image, -1, 1))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration: {i}, Total loss: {loss:.2f}, Content loss: {all_loss[0]:.2f}, Style loss: {all_loss[1]:.2f}\")\n",
    "\n",
    "    # Deprocess the final generated image\n",
    "    final_image = deprocess_image(generated_image.numpy())\n",
    "\n",
    "    return final_image\n",
    "\n",
    "# Specify the paths to the content and style images\n",
    "content_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\"\n",
    "style_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2478.jpg\"\n",
    "\n",
    "# Run the style transfer algorithm\n",
    "result_image = run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2)\n",
    "\n",
    "# Save and display the result image\n",
    "Image.fromarray(result_image).save('output_image.jpg')\n",
    "Image.fromarray(result_image).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the content image and style image\n",
    "content_image = Image.open(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\")\n",
    "style_image = Image.open(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\")\n",
    "\n",
    "# Preprocess the images\n",
    "preprocess = tf.keras.applications.vgg19.preprocess_input\n",
    "content_array = preprocess(np.expand_dims(content_image, axis=0))\n",
    "style_array = preprocess(np.expand_dims(style_image, axis=0))\n",
    "\n",
    "# Create a VGG19 model\n",
    "vgg19 = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Get the content and style feature maps\n",
    "content_features = vgg19(content_array)[\"block5_conv2\"]\n",
    "style_features = vgg19(style_array)[\"block5_conv2\"]\n",
    "\n",
    "# Define the target shape for the generated image\n",
    "target_shape = content_features.shape\n",
    "\n",
    "# Create a random generated image with the same shape as the target shape\n",
    "generated_image = tf.Variable(tf.random.uniform(target_shape, minval=0, maxval=255, dtype=tf.float32))\n",
    "\n",
    "# Define the content weight and style weight\n",
    "content_weight = 1e3\n",
    "style_weight = 1e-2\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=5)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(content_features, style_features, generated_features):\n",
    "    # Compute the content loss\n",
    "    content_loss = tf.reduce_mean(tf.square(content_features - generated_features))\n",
    "    \n",
    "    # Compute the style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean(tf.square(s - g)) for s, g in zip(style_features, generated_features)])\n",
    "    \n",
    "    return content_loss, style_loss\n",
    "\n",
    "@tf.function\n",
    "def compute_grads(content_features, style_features, generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_features = vgg19(generated_image)[\"block5_conv2\"]\n",
    "        loss = compute_loss(content_features, style_features, generated_features)\n",
    "    grads = tape.gradient(loss[0] * content_weight + loss[1] * style_weight, generated_image)\n",
    "    optimizer.apply_gradients([(grads, generated_image)])\n",
    "    return loss\n",
    "\n",
    "def run_style_transfer(content_image_path, style_image_path, num_iterations, content_weight, style_weight):\n",
    "    # Load the content and style images\n",
    "    content_image = Image.open(content_image_path)\n",
    "    style_image = Image.open(style_image_path)\n",
    "    \n",
    "    # Preprocess the images\n",
    "    preprocess = tf.keras.applications.vgg19.preprocess_input\n",
    "    content_array = preprocess(np.expand_dims(content_image, axis=0))\n",
    "    style_array = preprocess(np.expand_dims(style_image, axis=0))\n",
    "    \n",
    "    # Create a VGG19 model\n",
    "    vgg19 = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "    # Get the content and style feature maps\n",
    "    content_features = vgg19(content_array)[\"block5_conv2\"]\n",
    "    style_features = vgg19(style_array)[\"block5_conv2\"]\n",
    "    \n",
    "    # Define the target shape for the generated image\n",
    "    target_shape = content_features.shape\n",
    "    \n",
    "    # Create a random generated image with the same shape as the target shape\n",
    "    generated_image = tf.Variable(tf.random.uniform(target_shape, minval=0, maxval=255, dtype=tf.float32))\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=5)\n",
    "    \n",
    "    # Run the style transfer algorithm\n",
    "    for i in range(num_iterations):\n",
    "        loss = compute_grads(content_features, style_features, generated_image)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration: {i}, Content Loss: {loss[0]}, Style Loss: {loss[1]}\")\n",
    "    \n",
    "    # Convert the generated image to the expected range\n",
    "    generated_image = tf.clip_by_value(generated_image, 0, 255)\n",
    "    generated_image = tf.cast(generated_image, tf.uint8)\n",
    "    \n",
    "    # Return the generated image\n",
    "    return generated_image.numpy()\n",
    "\n",
    "# Set the paths and other parameters\n",
    "content_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\"\n",
    "style_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\"\n",
    "num_iterations = 1000\n",
    "content_weight = 1e3\n",
    "style_weight = 1e-2\n",
    "\n",
    "# Run the style transfer algorithm\n",
    "result_image = run_style_transfer(content_path, style_path, num_iterations, content_weight, style_weight)\n",
    "\n",
    "# Save and display the result image\n",
    "Image.fromarray(result_image[0]).save(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2478dundo.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
